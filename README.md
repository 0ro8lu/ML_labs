# ML_labs2022
This repository contains the laboratory exercises with discussions of the Machine Learning course (2022/23) at the Master's degree in Computer Science at Sapienza University of Rome.

The coding environment is Google Colab so that students don't have to configure a designated environment with specific Python packages.

The syllabus of the laboratory courses is:

1)	Data/feature pre-processing

   a.	Cleaning 
   
    i.	missing
    
    ii.	inconsistent
    
    iii.	noisy data 
    
  b.	Missing values
  
    i.	univariate vs multivariate
    
    ii.	nearest neighbours imputation
    
  c.	Scaling
  
    i.	Standard
    
    ii.	min-max
    
    iii.	max abs scaling
    
    iv.	mapping to uniform/Gaussian distribution
    
  d.	Normalisation
  
  e.	Encoding
  
    i.	encoding categorical features
    
  f.	Discretisation
  
    i.	k-bins 
    
    ii.	feature binarisation
    
  g.	Label balancing 
  
    i.	up-sample minority class
    
    ii.	down-sample majority class
    
    iii.	advanced classes balancing methods
    
2)	Evaluation

  a.	Confusion matrix
  
    i.	Example/exercise of binary classification
    
    ii.	Example/exercise of multi-class classification
    
  b.	Classification metrics
  
    i.	Binary classification
    
    ii.	Multi-class classification
    
    iii.	Micro/macro/weighted averages
    
  c.	Training and test data splitting
  
    i.	Pitfalls
    
    ii.	Cross-validation
    
    iii.	K-fold stratified cross-validation
    
3)	Ensembles and Neural Networks (12/12/2022)

  a.	Ensembles
  
    i.	Bagging meta-estimator
    
    ii.	Forests randomised trees
    
      -	Random Forest Classifier
      
      -	Extremely Randomised Trees
      
    iii.	AdaBoost
    
    iv.	Stacked ensembles
    
  b.	Multilayer Perceptron (MLP)
  
    i.	Classification
    
    ii.	Regression
    
    iii.	Regularisation
    
4)	Latent representation and embeddings

  a.	MNIST dataset as benchmarking system
  
  b.	Use BAE (GMU software) to build simple and boosting-based autoencoders
  
  c.	Variational Autoencoders (VAEs)
  
  d.	Vector Quantized Variational Autoencoders (VQ-VAEs)
  
  e.	Exemplar Autoencoder
  
  f.	Attention mechanisms and Transformers
  
5)	Hyperparameters tuning

  a.	Grid search -> drawbacks
  
  b.	Randomised search -> drawbacks
  
  c.	Bayesian optimisation -> advantages and drawbacks
  

