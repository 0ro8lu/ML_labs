{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gGy6fMw-gF4M",
        "AZUDXLNPlrYO",
        "UmNADiqjmGn7",
        "BX7Vgmdamo0H",
        "OFlM6wafm_Fi",
        "BJaD5ZEonLhm"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Machine Learning with Decision Trees, Random Forests, and XGBoost"
      ],
      "metadata": {
        "id": "gMqi3hglEEZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lesson Overview:\n",
        "In this lesson, we will cover the basics of machine learning using three popular algorithms: Decision Trees, Random Forests, and XGBoost. We'll start by exploring how to process data with both categorical and continuous features, followed by a step-by-step implementation of each algorithm."
      ],
      "metadata": {
        "id": "LmjzDqHOEHFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ucimlrepo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Fy_PHmHFico",
        "outputId": "0918fcf2-3f2a-451f-be75-11b79fbb9f50"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ucimlrepo\n",
            "  Downloading ucimlrepo-0.0.3-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: ucimlrepo\n",
            "Successfully installed ucimlrepo-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Processing\n",
        "\n",
        "In this section, we load the dataset, separate features and target variable, and encode categorical features using OneHotEncoding. This step prepares the data for training our machine learning models.\n"
      ],
      "metadata": {
        "id": "_CUuHEHLEhG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the Adult dataset\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# fetch dataset\n",
        "adult = fetch_ucirepo(id=2)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = adult.data.features\n",
        "y = adult.data.targets\n",
        "\n",
        "# variable information\n",
        "print(adult.variables)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5ahg1fGEd5_",
        "outputId": "e5be5e67-57f0-47fa-e825-7477bf5c5ea0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              name     role         type      demographic  \\\n",
            "0              age  Feature      Integer              Age   \n",
            "1        workclass  Feature  Categorical           Income   \n",
            "2           fnlwgt  Feature      Integer             None   \n",
            "3        education  Feature  Categorical  Education Level   \n",
            "4    education-num  Feature      Integer  Education Level   \n",
            "5   marital-status  Feature  Categorical            Other   \n",
            "6       occupation  Feature  Categorical            Other   \n",
            "7     relationship  Feature  Categorical            Other   \n",
            "8             race  Feature  Categorical             Race   \n",
            "9              sex  Feature       Binary              Sex   \n",
            "10    capital-gain  Feature      Integer             None   \n",
            "11    capital-loss  Feature      Integer             None   \n",
            "12  hours-per-week  Feature      Integer             None   \n",
            "13  native-country  Feature  Categorical            Other   \n",
            "14          income   Target       Binary           Income   \n",
            "\n",
            "                                          description units missing_values  \n",
            "0                                                 N/A  None             no  \n",
            "1   Private, Self-emp-not-inc, Self-emp-inc, Feder...  None            yes  \n",
            "2                                                None  None             no  \n",
            "3    Bachelors, Some-college, 11th, HS-grad, Prof-...  None             no  \n",
            "4                                                None  None             no  \n",
            "5   Married-civ-spouse, Divorced, Never-married, S...  None             no  \n",
            "6   Tech-support, Craft-repair, Other-service, Sal...  None            yes  \n",
            "7   Wife, Own-child, Husband, Not-in-family, Other...  None             no  \n",
            "8   White, Asian-Pac-Islander, Amer-Indian-Eskimo,...  None             no  \n",
            "9                                       Female, Male.  None             no  \n",
            "10                                               None  None             no  \n",
            "11                                               None  None             no  \n",
            "12                                               None  None             no  \n",
            "13  United-States, Cambodia, England, Puerto-Rico,...  None            yes  \n",
            "14                                       >50K, <=50K.  None             no  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Separate Features and Target Variable"
      ],
      "metadata": {
        "id": "Ua3yGshbE2dW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_adult, X_test_adult, y_train_adult, y_test_adult = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "UJRNVDpqEyAf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encode Categorical Features\n",
        "\n",
        "This modification uses the align method to ensure that the one-hot encoded DataFrames for the training and testing sets have the same columns. Any columns present in one set but not in the other will be added with zeros."
      ],
      "metadata": {
        "id": "JABL5MJyE6t_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode categorical features with column alignment\n",
        "X_train_adult = pd.get_dummies(X_train_adult)\n",
        "X_test_adult = pd.get_dummies(X_test_adult)\n",
        "\n",
        "# Align columns to ensure consistency between training and testing sets\n",
        "X_train_adult, X_test_adult = X_train_adult.align(X_test_adult, join='outer', axis=1, fill_value=0)"
      ],
      "metadata": {
        "id": "g_eHarvUE3z-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Trees\n",
        "\n",
        "A decision tree is a popular machine learning algorithm that is used for both classification and regression tasks. It models decisions as a tree-like structure where an input is progressively split into subsets based on certain features. Each internal node of the tree represents a decision based on a particular feature, and each leaf node represents the outcome or decision.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1.   **Root Node**: The tree starts with a root node that includes the entire dataset.\n",
        "2. **Feature Selection**: The algorithm evaluates different features in the dataset to determine the best feature to split the data. The goal is to find the feature that provides the best separation of the data into distinct classes or values.\n",
        "3. **Splitting**: The dataset is split into subsets based on the chosen feature. Each branch represents a possible outcome or decision based on the feature's value.\n",
        "4. **Recursive Process**: The splitting process is then applied recursively to each subset. At each internal node, a decision is made based on a specific feature, and the dataset is divided into subsets accordingly.\n",
        "5. **Stopping Criteria**: The recursive process continues until a stopping criteria is met. This could be a predefined depth of the tree, a minimum number of samples in a leaf node, or other criteria. These criteria help prevent overfitting.\n",
        "6. **Leaf Nodes**: The final nodes of the tree are called leaf nodes, and they represent the output or decision. For classification tasks, each leaf corresponds to a specific class, while for regression tasks, the leaf nodes contain the predicted values.\n",
        "7. **Prediction**: To make predictions for new data, you traverse the tree from the root to a leaf node based on the values of the input features. The predicted output is then based on the majority class (for classification) or the average value (for regression) of the samples in the leaf node.\n",
        "\n",
        "\n",
        "Decision trees are popular because they are easy to understand and interpret. However, they are prone to overfitting, and various techniques such as pruning and setting stopping criteria are used to mitigate this issue. Additionally, ensemble methods like Random Forests and Gradient Boosted Trees are often employed to improve the predictive performance of decision trees.\n"
      ],
      "metadata": {
        "id": "31sLwRk7G8oK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example\n",
        "Let's consider a simple example of a decision tree for a binary classification problem, where the goal is to determine whether a person will play golf based on weather conditions. The features are \"*Outlook*,\" \"*Temperature*,\" \"*Humidity*,\" and \"*Wind*.\"\n",
        "\n",
        "Decision Tree for Play Golf:\n",
        "\n",
        "                          Outlook\n",
        "                           / | \\\n",
        "                          /  |  \\\n",
        "                  Sunny   | Overcast | Rainy\n",
        "                         \\  /           \\\n",
        "                          \\/              \\\n",
        "                   Humidity <= 75         No\n",
        "                     /       \\\n",
        "                    /         \\\n",
        "                 Yes           No\n",
        "\n",
        "\n",
        "\n",
        "Here's a step-by-step breakdown:\n",
        "\n",
        "1. **Root Node**: The root node considers the feature \"Outlook.\"\n",
        "2. **Splitting at Outlook**: Three branches are created based on different outlook conditions: Sunny, Overcast, and Rainy.\n",
        "3. **Further Splitting**: For the \"Sunny\" branch, the decision is based on the \"Humidity\" feature. If humidity is less than or equal to 75, the decision is \"Yes\" (play golf), otherwise \"No.\" For the \"Overcast\" branch, the decision is \"Yes\" directly without further splitting. For the \"Rainy\" branch, no additional splitting is done, and the decision is \"No.\"\n",
        "4. **Leaf Nodes**: The leaf nodes represent the final decisions. In this case, \"Yes\" means play golf, and \"No\" means do not play golf.\n",
        "\n",
        "\n",
        "This simple decision tree provides a set of rules to decide whether a person will play golf based on the given weather conditions. Note that this is a basic example, and in a real-world scenario, decision trees can be more complex with additional features and nodes.**Also, it's important to consider overfitting and use techniques like pruning to optimize the tree's performance.**"
      ],
      "metadata": {
        "id": "xk_zRtqPKKe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import DecisionTreeClassifier from scikit-learn\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a decision tree model\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "dt_model.fit(X_train_adult, y_train_adult)\n",
        "\n",
        "# Make predictions on the test set\n",
        "dt_predictions = dt_model.predict(X_test_adult)\n",
        "\n",
        "# Evaluate the accuracy of the model\n",
        "dt_accuracy = accuracy_score(y_test_adult, dt_predictions)\n",
        "print(f'Test accuracy {dt_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQi5fRupE9pz",
        "outputId": "f60552f2-5b31-4829-9a55-6d36af611a0f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy 0.4677039615109018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forests"
      ],
      "metadata": {
        "id": "Jjo0dBSRHwcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import RandomForestClassifier from scikit-learn\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Create a random forest model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train_adult, y_train_adult.values.ravel())  # Use .values.ravel() to convert to 1-dimensional array\n",
        "\n",
        "# Make predictions on the test set\n",
        "rf_predictions = rf_model.predict(X_test_adult)\n",
        "\n",
        "# Evaluate the accuracy of the model\n",
        "rf_accuracy = accuracy_score(y_test_adult, rf_predictions)\n",
        "print(f'Test accuracy {rf_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WEFy69UHG8j",
        "outputId": "4faa3849-3ed0-46db-e74b-3135b78c6a82"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy 0.5371071757600573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost"
      ],
      "metadata": {
        "id": "1UAC2ZqQINqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install xgboost if not already installed\n",
        "# !pip install xgboost\n",
        "\n",
        "# Import XGBClassifier from xgboost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Import accuracy_score for evaluation\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create an XGBoost model\n",
        "xgb_model = XGBClassifier(random_state=42)\n",
        "\n",
        "# Encode the target variable using Label Encoding\n",
        "label_encoder_xgb = LabelEncoder()\n",
        "y_train_adult_encoded = label_encoder_xgb.fit_transform(y_train_adult)\n",
        "y_test_adult_encoded = label_encoder_xgb.transform(y_test_adult)\n",
        "\n",
        "# Train the model\n",
        "xgb_model.fit(X_train_adult, y_train_adult_encoded.ravel())\n",
        "\n",
        "# Make predictions on the test set\n",
        "xgb_predictions = xgb_model.predict(X_test_adult)\n",
        "\n",
        "# Decode the predictions back to original labels if needed\n",
        "xgb_predictions_original_labels = label_encoder_xgb.inverse_transform(xgb_predictions)\n",
        "\n",
        "# Evaluate the accuracy of the model\n",
        "xgb_accuracy = accuracy_score(y_test_adult, xgb_predictions_original_labels)\n",
        "print(f'Test accuracy {xgb_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OF7ZXTBH5RG",
        "outputId": "7724dc49-2700-4f54-e8a0-b655b24cd796"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy 0.593100624424199\n"
          ]
        }
      ]
    }
  ]
}